#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DXF ê³ ê¸‰ ë¶„ì„ê¸° - ìƒìš©í™” ìˆ˜ì¤€ì˜ ì „ë¬¸ ë¶„ì„ ê¸°ëŠ¥
Author: CAD Analysis Expert
Version: 2.0.0
License: MIT
"""

import math
import statistics
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any, Optional
import numpy as np
from pathlib import Path
import json
import re
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class DXFAdvancedAnalyzer:
    """ê³ ê¸‰ DXF ë¶„ì„ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.quality_metrics = {}
        self.complexity_metrics = {}
        self.standards_compliance = {}
        self.anomalies = []
        self.patterns = {}
        self.ai_context = {}
        
    def analyze_drawing_quality(self, analyzer_data: Dict) -> Dict:
        """ë„ë©´ í’ˆì§ˆ í‰ê°€"""
        quality_score = 100
        quality_issues = []
        
        # 1. ë ˆì´ì–´ êµ¬ì„± í‰ê°€
        layers = analyzer_data.get('layers', [])
        layer_score, layer_issues = self._evaluate_layer_organization(layers)
        quality_score -= (100 - layer_score) * 0.2
        quality_issues.extend(layer_issues)
        
        # 2. ì¹˜ìˆ˜ ì •í™•ë„ í‰ê°€
        dimensions = analyzer_data.get('dimensions', [])
        dim_score, dim_issues = self._evaluate_dimensions(dimensions)
        quality_score -= (100 - dim_score) * 0.3
        quality_issues.extend(dim_issues)
        
        # 3. ê°ì²´ ë¶„í¬ í‰ê°€
        entity_breakdown = analyzer_data.get('entity_breakdown', {})
        dist_score, dist_issues = self._evaluate_entity_distribution(entity_breakdown)
        quality_score -= (100 - dist_score) * 0.1
        quality_issues.extend(dist_issues)
        
        # 4. í…ìŠ¤íŠ¸ ì¼ê´€ì„± í‰ê°€
        texts = analyzer_data.get('texts', [])
        text_score, text_issues = self._evaluate_text_consistency(texts)
        quality_score -= (100 - text_score) * 0.1
        quality_issues.extend(text_issues)
        
        # 5. ë„ë©´ ë³µì¡ë„ í‰ê°€
        complexity = self.calculate_drawing_complexity(analyzer_data)
        overall_complexity = complexity.get('overall_complexity', 0)
        if overall_complexity is not None and overall_complexity > 0.8:
            quality_score -= 10
            quality_issues.append({
                'severity': 'warning',
                'category': 'ë³µì¡ë„',
                'message': 'ë„ë©´ì´ ë§¤ìš° ë³µì¡í•©ë‹ˆë‹¤. ê°€ë…ì„± ê°œì„ ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
            })
        
        self.quality_metrics = {
            'overall_score': max(0, quality_score),
            'layer_score': layer_score,
            'dimension_score': dim_score,
            'distribution_score': dist_score,
            'text_score': text_score,
            'issues': quality_issues,
            'grade': self._get_quality_grade(quality_score)
        }
        
        return self.quality_metrics
    
    def _evaluate_layer_organization(self, layers: List[Dict]) -> Tuple[float, List]:
        """ë ˆì´ì–´ êµ¬ì„± í‰ê°€"""
        score = 100
        issues = []
        
        if not layers:
            return 0, [{'severity': 'error', 'category': 'ë ˆì´ì–´', 'message': 'ë ˆì´ì–´ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}]
        
        # ë ˆì´ì–´ ëª…ëª… ê·œì¹™ ê²€ì‚¬
        layer_names = [layer['name'] for layer in layers]
        
        # ê¸°ë³¸ ë ˆì´ì–´(0)ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        if len(layer_names) == 1 and layer_names[0] == '0':
            score -= 30
            issues.append({
                'severity': 'warning',
                'category': 'ë ˆì´ì–´',
                'message': 'ê¸°ë³¸ ë ˆì´ì–´(0)ë§Œ ì‚¬ìš©ì¤‘ì…ë‹ˆë‹¤. ë ˆì´ì–´ êµ¬ë¶„ì´ í•„ìš”í•©ë‹ˆë‹¤.'
            })
        
        # ì˜ë¯¸ìˆëŠ” ë ˆì´ì–´ëª… ì‚¬ìš© ì—¬ë¶€
        meaningful_patterns = [
            r'ì¹˜ìˆ˜|dimension|dim',
            r'ì¤‘ì‹¬ì„ |center|cen',
            r'ìˆ¨ê¹€ì„ |hidden|hid',
            r'í…ìŠ¤íŠ¸|text|txt',
            r'í•´ì¹˜|hatch|hat'
        ]
        
        meaningful_count = sum(1 for name in layer_names 
                              if any(re.search(pattern, name, re.IGNORECASE) 
                                    for pattern in meaningful_patterns))
        
        if meaningful_count < len(layer_names) * 0.5:
            score -= 20
            issues.append({
                'severity': 'info',
                'category': 'ë ˆì´ì–´',
                'message': 'ë ˆì´ì–´ëª…ì´ ì²´ê³„ì ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. í‘œì¤€ ëª…ëª… ê·œì¹™ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.'
            })
        
        return score, issues
    
    def _evaluate_dimensions(self, dimensions: List[Dict]) -> Tuple[float, List]:
        """ì¹˜ìˆ˜ ì •í™•ë„ í‰ê°€"""
        score = 100
        issues = []
        
        if not dimensions:
            return score, issues
        
        # ì¹˜ìˆ˜ê°’ ë¶„ì„ (None ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        measurements = []
        for dim in dimensions:
            measurement = dim.get('measurement')
            if measurement is not None and isinstance(measurement, (int, float)) and measurement > 0:
                measurements.append(measurement)
        
        if measurements and len(measurements) > 1:
            try:
                # ê·¹ë‹¨ì ì¸ ê°’ ê²€ì‚¬
                mean_val = statistics.mean(measurements)
                std_val = statistics.stdev(measurements) if len(measurements) > 1 else 0
                
                outliers = []
                if std_val > 0:
                    outliers = [m for m in measurements 
                               if abs(m - mean_val) > 3 * std_val]
                
                if outliers:
                    score -= min(20, len(outliers) * 5)
                    issues.append({
                        'severity': 'warning',
                        'category': 'ì¹˜ìˆ˜',
                        'message': f'{len(outliers)}ê°œì˜ ì´ìƒ ì¹˜ìˆ˜ê°’ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.'
                    })
            except Exception as e:
                logger.warning(f"ì¹˜ìˆ˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return score, issues
    
    def _evaluate_entity_distribution(self, entity_breakdown: Dict) -> Tuple[float, List]:
        """ê°ì²´ ë¶„í¬ í‰ê°€"""
        score = 100
        issues = []
        
        if not entity_breakdown:
            return 0, [{'severity': 'error', 'category': 'ê°ì²´', 'message': 'ê°ì²´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'}]
        
        total_entities = sum(entity_breakdown.values())
        
        # íŠ¹ì • ê°ì²´ ìœ í˜•ì´ ê³¼ë„í•˜ê²Œ ë§ì€ ê²½ìš°
        for entity_type, count in entity_breakdown.items():
            ratio = count / total_entities
            
            if ratio > 0.7:
                score -= 15
                issues.append({
                    'severity': 'info',
                    'category': 'ê°ì²´ ë¶„í¬',
                    'message': f'{entity_type} ê°ì²´ê°€ ì „ì²´ì˜ {ratio*100:.1f}%ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤.'
                })
        
        return score, issues
    
    def _evaluate_text_consistency(self, texts: List[Dict]) -> Tuple[float, List]:
        """í…ìŠ¤íŠ¸ ì¼ê´€ì„± í‰ê°€"""
        score = 100
        issues = []
        
        if not texts:
            return score, issues
        
        # í…ìŠ¤íŠ¸ ë†’ì´ ì¼ê´€ì„± ê²€ì‚¬ (None ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        heights = []
        for text in texts:
            height = text.get('height')
            if height is not None and isinstance(height, (int, float)) and height > 0:
                heights.append(height)
        
        if heights:
            unique_heights = set(heights)
            
            if len(unique_heights) > 5:
                score -= 10
                issues.append({
                    'severity': 'info',
                    'category': 'í…ìŠ¤íŠ¸',
                    'message': f'{len(unique_heights)}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ë†’ì´ê°€ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.'
                })
        
        return score, issues
    
    def _get_quality_grade(self, score: float) -> str:
        """í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ë¶€ì—¬"""
        if score >= 90:
            return 'A (ìš°ìˆ˜)'
        elif score >= 80:
            return 'B (ì–‘í˜¸)'
        elif score >= 70:
            return 'C (ë³´í†µ)'
        elif score >= 60:
            return 'D (ë¯¸í¡)'
        else:
            return 'F (ê°œì„ í•„ìš”)'
    
    def calculate_drawing_complexity(self, analyzer_data: Dict) -> Dict:
        """ë„ë©´ ë³µì¡ë„ ê³„ì‚°"""
        total_entities = analyzer_data.get('summary_info', {}).get('total_entities', 0)
        layer_count = analyzer_data.get('summary_info', {}).get('layer_count', 0)
        entity_types = len(analyzer_data.get('entity_breakdown', {}))
        
        # ë³µì¡ë„ ìš”ì†Œë“¤
        entity_complexity = min(total_entities / 10000, 1.0)  # 10000ê°œ ê¸°ì¤€
        layer_complexity = min(layer_count / 50, 1.0)  # 50ê°œ ë ˆì´ì–´ ê¸°ì¤€
        type_complexity = min(entity_types / 20, 1.0)  # 20ì¢…ë¥˜ ê¸°ì¤€
        
        # ê°€ì¤‘ í‰ê· 
        overall_complexity = (
            entity_complexity * 0.5 +
            layer_complexity * 0.3 +
            type_complexity * 0.2
        )
        
        self.complexity_metrics = {
            'overall_complexity': overall_complexity,
            'entity_complexity': entity_complexity,
            'layer_complexity': layer_complexity,
            'type_complexity': type_complexity,
            'complexity_level': self._get_complexity_level(overall_complexity)
        }
        
        return self.complexity_metrics
    
    def _get_complexity_level(self, complexity: float) -> str:
        """ë³µì¡ë„ ìˆ˜ì¤€ íŒë‹¨"""
        if complexity < 0.2:
            return 'ë§¤ìš° ë‹¨ìˆœ'
        elif complexity < 0.4:
            return 'ë‹¨ìˆœ'
        elif complexity < 0.6:
            return 'ë³´í†µ'
        elif complexity < 0.8:
            return 'ë³µì¡'
        else:
            return 'ë§¤ìš° ë³µì¡'
    
    def check_standards_compliance(self, analyzer_data: Dict, standard: str = 'ISO') -> Dict:
        """ì„¤ê³„ í‘œì¤€ ì¤€ìˆ˜ ê²€ì¦"""
        compliance_score = 100
        violations = []
        
        # ISO/KS í‘œì¤€ ê²€ì¦
        if standard == 'ISO':
            # ë ˆì´ì–´ëª… í‘œì¤€
            layer_violations = self._check_iso_layer_standards(analyzer_data.get('layers', []))
            violations.extend(layer_violations)
            compliance_score -= len(layer_violations) * 5
            
            # ì„ ì¢…ë¥˜ í‘œì¤€
            linetype_violations = self._check_linetype_standards(analyzer_data.get('layers', []))
            violations.extend(linetype_violations)
            compliance_score -= len(linetype_violations) * 3
            
            # ì¹˜ìˆ˜ í‘œì¤€
            dim_violations = self._check_dimension_standards(analyzer_data.get('dimensions', []))
            violations.extend(dim_violations)
            compliance_score -= len(dim_violations) * 4
        
        self.standards_compliance = {
            'standard': standard,
            'compliance_score': max(0, compliance_score),
            'violations': violations,
            'compliant': compliance_score >= 80
        }
        
        return self.standards_compliance
    
    def _check_iso_layer_standards(self, layers: List[Dict]) -> List[Dict]:
        """ISO ë ˆì´ì–´ í‘œì¤€ ê²€ì¦"""
        violations = []
        
        # ISO í‘œì¤€ ë ˆì´ì–´ ìƒ‰ìƒ
        iso_colors = {
            'ì™¸ê³½ì„ ': [7, 'white'],
            'ì¤‘ì‹¬ì„ ': [1, 'red'],
            'ìˆ¨ê¹€ì„ ': [3, 'green'],
            'ì¹˜ìˆ˜ì„ ': [2, 'yellow'],
            'ì ˆë‹¨ì„ ': [4, 'cyan']
        }
        
        for layer in layers:
            layer_name = layer['name'].lower()
            layer_color = layer.get('color', 7)
            
            # í‘œì¤€ ë ˆì´ì–´ëª… ê²€ì‚¬
            for std_name, (std_color, color_name) in iso_colors.items():
                if std_name in layer_name and layer_color != std_color:
                    violations.append({
                        'type': 'layer_color',
                        'severity': 'warning',
                        'message': f"ë ˆì´ì–´ '{layer['name']}'ì˜ ìƒ‰ìƒì´ ISO í‘œì¤€ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (ê¶Œì¥: {color_name})"
                    })
        
        return violations
    
    def _check_linetype_standards(self, layers: List[Dict]) -> List[Dict]:
        """ì„ ì¢…ë¥˜ í‘œì¤€ ê²€ì¦"""
        violations = []
        
        # í‘œì¤€ ì„ ì¢…ë¥˜
        standard_linetypes = {
            'ì¤‘ì‹¬ì„ ': ['CENTER', 'DASHDOT'],
            'ìˆ¨ê¹€ì„ ': ['HIDDEN', 'DASHED'],
            'ì™¸ê³½ì„ ': ['CONTINUOUS']
        }
        
        for layer in layers:
            layer_name = layer['name'].lower()
            linetype = layer.get('linetype', 'CONTINUOUS').upper()
            
            for line_type, valid_types in standard_linetypes.items():
                if line_type in layer_name and linetype not in valid_types:
                    violations.append({
                        'type': 'linetype',
                        'severity': 'info',
                        'message': f"ë ˆì´ì–´ '{layer['name']}'ì˜ ì„ ì¢…ë¥˜ê°€ í‘œì¤€ê³¼ ë‹¤ë¦…ë‹ˆë‹¤."
                    })
        
        return violations
    
    def _check_dimension_standards(self, dimensions: List[Dict]) -> List[Dict]:
        """ì¹˜ìˆ˜ í‘œì¤€ ê²€ì¦"""
        violations = []
        
        # ì¹˜ìˆ˜ í…ìŠ¤íŠ¸ ë†’ì´ í‘œì¤€ (mm)
        standard_heights = [2.5, 3.5, 5.0, 7.0]
        
        for dim in dimensions:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì¹˜ìˆ˜ í…ìŠ¤íŠ¸ ë†’ì´ ë“±ì„ ê²€ì‚¬
            pass
        
        return violations
    
    def detect_anomalies(self, analyzer_data: Dict) -> List[Dict]:
        """ë„ë©´ ì´ìƒ ì§•í›„ íƒì§€"""
        anomalies = []
        
        # 1. ì¤‘ë³µ ê°ì²´ íƒì§€
        duplicates = self._detect_duplicate_entities(analyzer_data)
        anomalies.extend(duplicates)
        
        # 2. ê³ ë¦½ëœ ê°ì²´ íƒì§€
        isolated = self._detect_isolated_entities(analyzer_data)
        anomalies.extend(isolated)
        
        # 3. ë¹„ì •ìƒì ì¸ ìŠ¤ì¼€ì¼ íƒì§€
        scale_issues = self._detect_scale_anomalies(analyzer_data)
        anomalies.extend(scale_issues)
        
        self.anomalies = anomalies
        return anomalies
    
    def _detect_duplicate_entities(self, analyzer_data: Dict) -> List[Dict]:
        """ì¤‘ë³µ ê°ì²´ íƒì§€"""
        anomalies = []
        
        # ì›/í˜¸ ì¤‘ë³µ ê²€ì‚¬ (None ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        circles = analyzer_data.get('circles', [])
        if len(circles) > 1:
            # ë™ì¼ ìœ„ì¹˜, ë™ì¼ ë°˜ì§€ë¦„ ì› ì°¾ê¸°
            seen = {}
            for circle in circles:
                try:
                    center = circle.get('center')
                    radius = circle.get('radius')
                    
                    if (center is not None and len(center) >= 2 and 
                        radius is not None and isinstance(radius, (int, float))):
                        
                        key = (
                            round(float(center[0]), 3),
                            round(float(center[1]), 3),
                            round(float(radius), 3)
                        )
                        
                        if key in seen:
                            anomalies.append({
                                'type': 'duplicate_circle',
                                'severity': 'warning',
                                'message': f"ì¤‘ë³µëœ ì›ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ì¹˜: {center}"
                            })
                        else:
                            seen[key] = True
                except Exception as e:
                    logger.warning(f"ì› ì¤‘ë³µ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        return anomalies
    
    def _detect_isolated_entities(self, analyzer_data: Dict) -> List[Dict]:
        """ê³ ë¦½ëœ ê°ì²´ íƒì§€"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê³µê°„ ë¶„ì„ì„ í†µí•´ ê³ ë¦½ëœ ê°ì²´ íƒì§€
        return []
    
    def _detect_scale_anomalies(self, analyzer_data: Dict) -> List[Dict]:
        """ìŠ¤ì¼€ì¼ ì´ìƒ íƒì§€"""
        anomalies = []
        
        # í…ìŠ¤íŠ¸ í¬ê¸° ì´ìƒ íƒì§€ (None ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        texts = analyzer_data.get('texts', [])
        if texts:
            # ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë†’ì´ë§Œ ìˆ˜ì§‘
            valid_heights = []
            for text in texts:
                height = text.get('height')
                if height is not None and isinstance(height, (int, float)) and height > 0:
                    valid_heights.append(height)
            
            if valid_heights and len(valid_heights) > 1:
                try:
                    mean_height = statistics.mean(valid_heights)
                    
                    # í‰ê· ì˜ 10ë°° ì´ìƒ ë˜ëŠ” 1/10 ì´í•˜ì¸ í…ìŠ¤íŠ¸ ì°¾ê¸°
                    for text in texts:
                        height = text.get('height')
                        if (height is not None and isinstance(height, (int, float)) and 
                            height > 0 and mean_height > 0):
                            if height > mean_height * 10 or height < mean_height / 10:
                                anomalies.append({
                                    'type': 'text_scale',
                                    'severity': 'info',
                                    'message': f"ë¹„ì •ìƒì ì¸ í…ìŠ¤íŠ¸ í¬ê¸°: {height:.2f} (í‰ê· : {mean_height:.2f})"
                                })
                except Exception as e:
                    logger.warning(f"í…ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return anomalies
    
    def detect_patterns(self, analyzer_data: Dict) -> Dict:
        """ë„ë©´ íŒ¨í„´ ë¶„ì„"""
        patterns = {
            'repeated_dimensions': self._find_repeated_dimensions(analyzer_data),
            'grid_pattern': self._detect_grid_pattern(analyzer_data),
            'symmetry': self._detect_symmetry(analyzer_data)
        }
        
        self.patterns = patterns
        return patterns
    
    def _find_repeated_dimensions(self, analyzer_data: Dict) -> Dict:
        """ë°˜ë³µë˜ëŠ” ì¹˜ìˆ˜ íŒ¨í„´ ì°¾ê¸°"""
        dimensions = analyzer_data.get('dimensions', [])
        
        if not dimensions:
            return {'found': False}
        
        # ì¹˜ìˆ˜ê°’ ë¹ˆë„ ë¶„ì„ (None ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        measurements = []
        for dim in dimensions:
            measurement = dim.get('measurement')
            if measurement is not None and isinstance(measurement, (int, float)) and measurement > 0:
                measurements.append(measurement)
        
        if not measurements:
            return {'found': False}
        
        try:
            freq = Counter(measurements)
            repeated = {k: v for k, v in freq.items() if v > 1}
            
            return {
                'found': bool(repeated),
                'repeated_values': repeated,
                'most_common': freq.most_common(5)
            }
        except Exception as e:
            logger.warning(f"ë°˜ë³µ ì¹˜ìˆ˜ íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'found': False}
    
    def _detect_grid_pattern(self, analyzer_data: Dict) -> Dict:
        """ê²©ì íŒ¨í„´ íƒì§€"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì¢Œí‘œ ë¶„ì„ì„ í†µí•œ ê²©ì íŒ¨í„´ íƒì§€
        return {'found': False}
    
    def _detect_symmetry(self, analyzer_data: Dict) -> Dict:
        """ëŒ€ì¹­ì„± íƒì§€"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê°ì²´ ìœ„ì¹˜ ë¶„ì„ì„ í†µí•œ ëŒ€ì¹­ì„± íƒì§€
        return {'found': False}
    
    def generate_ai_context(self, analyzer_data: Dict) -> Dict:
        """AI ë¶„ì„ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        # í’ˆì§ˆ ë¶„ì„
        quality = self.analyze_drawing_quality(analyzer_data)
        
        # ë³µì¡ë„ ë¶„ì„
        complexity = self.calculate_drawing_complexity(analyzer_data)
        
        # í‘œì¤€ ì¤€ìˆ˜ ê²€ì¦
        compliance = self.check_standards_compliance(analyzer_data)
        
        # ì´ìƒ ì§•í›„ íƒì§€
        anomalies = self.detect_anomalies(analyzer_data)
        
        # íŒ¨í„´ ë¶„ì„
        patterns = self.detect_patterns(analyzer_data)
        
        # AIë¥¼ ìœ„í•œ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸
        self.ai_context = {
            'summary': {
                'file_name': analyzer_data.get('file_info', {}).get('filename', 'unknown'),
                'total_entities': analyzer_data.get('summary_info', {}).get('total_entities', 0),
                'quality_score': quality['overall_score'],
                'quality_grade': quality['grade'],
                'complexity_level': complexity['complexity_level'],
                'standards_compliant': compliance['compliant']
            },
            'quality_analysis': {
                'score': quality['overall_score'],
                'issues': quality['issues'],
                'recommendations': self._generate_quality_recommendations(quality)
            },
            'complexity_analysis': complexity,
            'standards_compliance': compliance,
            'anomalies': anomalies,
            'patterns': patterns,
            'key_insights': self._generate_key_insights(
                quality, complexity, compliance, anomalies, patterns
            ),
            'improvement_suggestions': self._generate_improvement_suggestions(
                quality, complexity, compliance, anomalies
            ),
            # AI ëª¨ë¸ì´ ì„¸ë°€í•œ íŒë‹¨ì„ í•  ìˆ˜ ìˆë„ë¡ ì›ë³¸ ë„ë©´ ë°ì´í„° ì „ì²´ë¥¼ í¬í•¨ (ìš©ëŸ‰ ì£¼ì˜)
            'raw_data': analyzer_data
        }
        
        return self.ai_context
    
    def _generate_quality_recommendations(self, quality_metrics: Dict) -> List[str]:
        """í’ˆì§ˆ ê°œì„  ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if quality_metrics['layer_score'] < 80:
            recommendations.append("ë ˆì´ì–´ êµ¬ì„±ì„ ì²´ê³„í™”í•˜ê³  í‘œì¤€ ëª…ëª… ê·œì¹™ì„ ì ìš©í•˜ì„¸ìš”.")
        
        if quality_metrics['dimension_score'] < 80:
            recommendations.append("ì¹˜ìˆ˜ ì •í™•ë„ë¥¼ ê²€í† í•˜ê³  ì´ìƒê°’ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        if quality_metrics['text_score'] < 80:
            recommendations.append("í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ì„ í†µì¼í•˜ì—¬ ì¼ê´€ì„±ì„ ë†’ì´ì„¸ìš”.")
        
        return recommendations
    
    def _generate_key_insights(self, quality, complexity, compliance, anomalies, patterns) -> List[str]:
        """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸
        if quality['overall_score'] < 70:
            insights.append(f"ë„ë©´ í’ˆì§ˆì´ {quality['grade']} ë“±ê¸‰ìœ¼ë¡œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif quality['overall_score'] > 90:
            insights.append("ë„ë©´ í’ˆì§ˆì´ ìš°ìˆ˜í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
        
        # ë³µì¡ë„ ì¸ì‚¬ì´íŠ¸
        overall_complexity = complexity.get('overall_complexity', 0)
        if overall_complexity is not None and overall_complexity > 0.7:
            insights.append("ë„ë©´ì´ ë³µì¡í•˜ì—¬ ê°€ë…ì„± ê°œì„ ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # í‘œì¤€ ì¤€ìˆ˜ ì¸ì‚¬ì´íŠ¸
        if not compliance['compliant']:
            insights.append(f"{len(compliance['violations'])}ê°œì˜ í‘œì¤€ ìœ„ë°˜ ì‚¬í•­ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì´ìƒ ì§•í›„ ì¸ì‚¬ì´íŠ¸
        if anomalies:
            insights.append(f"{len(anomalies)}ê°œì˜ ì´ìƒ ì§•í›„ê°€ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # íŒ¨í„´ ì¸ì‚¬ì´íŠ¸
        if patterns.get('repeated_dimensions', {}).get('found'):
            insights.append("ë°˜ë³µë˜ëŠ” ì¹˜ìˆ˜ íŒ¨í„´ì´ ë°œê²¬ë˜ì–´ í‘œì¤€í™” ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        
        return insights
    
    def _generate_improvement_suggestions(self, quality, complexity, compliance, anomalies) -> List[Dict]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        # ìš°ì„ ìˆœìœ„ë³„ ê°œì„  ì œì•ˆ
        if quality['overall_score'] < 60:
            suggestions.append({
                'priority': 'high',
                'category': 'í’ˆì§ˆ',
                'suggestion': 'ë„ë©´ í’ˆì§ˆ ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤. ë ˆì´ì–´ êµ¬ì„±ê³¼ ê°ì²´ ì •ë¦¬ë¥¼ ìš°ì„  ì§„í–‰í•˜ì„¸ìš”.',
                'impact': 'ê°€ë…ì„± ë° ìœ ì§€ë³´ìˆ˜ì„± í¬ê²Œ í–¥ìƒ'
            })
        
        overall_complexity = complexity.get('overall_complexity', 0)
        if overall_complexity is not None and overall_complexity > 0.8:
            suggestions.append({
                'priority': 'medium',
                'category': 'ë³µì¡ë„',
                'suggestion': 'ë„ë©´ì„ ì—¬ëŸ¬ ì‹œíŠ¸ë¡œ ë¶„í• í•˜ê±°ë‚˜ ìƒì„¸ë„ë¥¼ ë³„ë„ë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.',
                'impact': 'ì´í•´ë„ í–¥ìƒ ë° ì˜¤ë¥˜ ê°ì†Œ'
            })
        
        if not compliance['compliant']:
            suggestions.append({
                'priority': 'medium',
                'category': 'í‘œì¤€',
                'suggestion': 'ISO/KS í‘œì¤€ì— ë§ì¶° ë ˆì´ì–´ëª…ê³¼ ì„ ì¢…ë¥˜ë¥¼ ì¡°ì •í•˜ì„¸ìš”.',
                'impact': 'í˜¸í™˜ì„± í–¥ìƒ ë° í˜‘ì—… íš¨ìœ¨ì„± ì¦ëŒ€'
            })
        
        if anomalies:
            suggestions.append({
                'priority': 'high',
                'category': 'ì´ìƒì§•í›„',
                'suggestion': 'íƒì§€ëœ ì´ìƒ ì§•í›„ë“¤ì„ ê²€í† í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”.',
                'impact': 'ë„ë©´ ì •í™•ë„ í–¥ìƒ'
            })
        
        return sorted(suggestions, key=lambda x: {'high': 0, 'medium': 1, 'low': 2}[x['priority']])
    
    def export_for_ai(self, analyzer_data: Dict, format: str = 'markdown') -> str:
        """AI ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        context = self.generate_ai_context(analyzer_data)
        
        if format == 'markdown':
            return self._export_as_markdown(context, analyzer_data)
        elif format == 'json':
            return json.dumps(context, ensure_ascii=False, indent=2)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
    
    def _export_as_markdown(self, context: Dict, analyzer_data: Dict) -> str:
        """AIìš© ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        md_content = f"""# DXF ë„ë©´ ê³ ê¸‰ ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ“‹ ê¸°ë³¸ ì •ë³´
- **íŒŒì¼ëª…**: {context['summary']['file_name']}
- **ì „ì²´ ê°ì²´ ìˆ˜**: {context['summary']['total_entities']:,}
- **ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ¯ ì¢…í•© í‰ê°€
- **í’ˆì§ˆ ë“±ê¸‰**: {context['summary']['quality_grade']} ({context['summary']['quality_score']:.1f}ì )
- **ë³µì¡ë„**: {context['summary']['complexity_level']}
- **í‘œì¤€ ì¤€ìˆ˜**: {'ì¤€ìˆ˜' if context['summary']['standards_compliant'] else 'ë¯¸ì¤€ìˆ˜'}

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸
"""
        for insight in context['key_insights']:
            md_content += f"- {insight}\n"
        
        md_content += "\n## ğŸ” ìƒì„¸ ë¶„ì„\n\n"
        
        # í’ˆì§ˆ ë¶„ì„
        md_content += "### í’ˆì§ˆ ë¶„ì„\n"
        if context['quality_analysis']['issues']:
            md_content += "**ë°œê²¬ëœ ë¬¸ì œì :**\n"
            for issue in context['quality_analysis']['issues']:
                md_content += f"- [{issue['severity']}] {issue['category']}: {issue['message']}\n"
        
        if context['quality_analysis']['recommendations']:
            md_content += "\n**ê°œì„  ê¶Œê³ ì‚¬í•­:**\n"
            for rec in context['quality_analysis']['recommendations']:
                md_content += f"- {rec}\n"
        
        # ì´ìƒ ì§•í›„
        if context['anomalies']:
            md_content += "\n### ì´ìƒ ì§•í›„\n"
            for anomaly in context['anomalies']:
                md_content += f"- [{anomaly['severity']}] {anomaly['type']}: {anomaly['message']}\n"
        
        # ê°œì„  ì œì•ˆ
        md_content += "\n## ğŸ“ˆ ê°œì„  ì œì•ˆ\n"
        for suggestion in context['improvement_suggestions']:
            md_content += f"\n### {suggestion['priority'].upper()} - {suggestion['category']}\n"
            md_content += f"**ì œì•ˆ**: {suggestion['suggestion']}\n"
            md_content += f"**ê¸°ëŒ€ íš¨ê³¼**: {suggestion['impact']}\n"
        
        # AI í”„ë¡¬í”„íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸
        md_content += "\n## ğŸ¤– AI ë¶„ì„ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸\n"
        md_content += "```json\n"
        md_content += json.dumps(context, ensure_ascii=False, indent=2)
        md_content += "\n```\n"
        
        # ---- ì¶”ê°€ ìƒì„¸ í…Œì´ë¸” (ì¹˜ìˆ˜ & ì›) ----
        # ë„ˆë¬´ í° ë„ë©´ì˜ ê²½ìš° í‘œê°€ ê³¼ë„í•˜ê²Œ ê¸¸ì–´ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ìµœëŒ€ 100í–‰ê¹Œì§€ë§Œ í‘œì‹œ
        dim_max = 100
        circ_max = 100

        dimensions = analyzer_data.get('dimensions', [])
        if dimensions:
            md_content += "\n---\n\n### ğŸ“ ì¹˜ìˆ˜ (DIMENSION) ìƒì„¸\n\n"
            md_content += "| No. | ì¸¡ì •ê°’ | ë„ë©´ í‘œê¸° í…ìŠ¤íŠ¸ | ì¹˜ìˆ˜ ìŠ¤íƒ€ì¼ | ë ˆì´ì–´ |\n"
            md_content += "|-----|--------|-----------------|-------------|--------|\n"
            for idx, dim in enumerate(dimensions[:dim_max], start=1):
                text_val = str(dim.get('text', '')).replace('|', '\\|').replace('\n', ' ')
                md_content += f"| {idx} | {dim.get('measurement', '')} | {text_val} | {dim.get('style', '')} | {dim.get('layer', '')} |\n"
            if len(dimensions) > dim_max:
                md_content += f"| ... | ... | (ì´ {len(dimensions)}ê°œ) | ... | ... | ... |\n"

        circles = analyzer_data.get('circles', [])
        if circles:
            md_content += "\n### ğŸ”µ ì› (CIRCLE) ìƒì„¸\n\n"
            md_content += "| No. | ì¤‘ì‹¬ì  (X, Y, Z) | ë°˜ì§€ë¦„ | ì§€ë¦„ | ë ˆì´ì–´ |\n"
            md_content += "|-----|--------------------|--------|------|--------|\n"
            for idx, circ in enumerate(circles[:circ_max], start=1):
                cx, cy, cz = circ.get('center', (0, 0, 0))
                md_content += f"| {idx} | ({cx:.3f}, {cy:.3f}, {cz:.3f}) | {circ.get('radius', 0):.3f} | {circ.get('diameter', 0):.3f} | {circ.get('layer', '')} |\n"
            if len(circles) > circ_max:
                md_content += f"| ... | ... | ... | (ì´ {len(circles)}ê°œ) | ... |\n"

        md_content += "\n"
        
        return md_content 