#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DXF AI í†µí•© ëª¨ë“ˆ - OpenAI, Claude ë° Gemini API ì—°ë™
Author: AI Integration Expert
Version: 2.1.0
"""

import os
import json
from typing import Dict, List, Optional, Union
import logging
from datetime import datetime
import asyncio
import aiohttp

# OpenAI
import openai

# Claude (Anthropic)
try:
    import anthropic
    CLAUDE_AVAILABLE = True
except ImportError:
    CLAUDE_AVAILABLE = False

# Google Gemini
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    
logger = logging.getLogger(__name__)


class DXFAIIntegration:
    """DXF ë¶„ì„ AI í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # API í‚¤ ì„¤ì •
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.claude_api_key = os.getenv('ANTHROPIC_API_KEY')
        self.gemini_api_key = os.getenv('GOOGLE_API_KEY')
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if self.openai_api_key:
            openai.api_key = self.openai_api_key
            
        if self.claude_api_key and CLAUDE_AVAILABLE:
            self.claude_client = anthropic.Anthropic(api_key=self.claude_api_key)
        else:
            self.claude_client = None
            
        if self.gemini_api_key and GEMINI_AVAILABLE:
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-pro')
        else:
            self.gemini_model = None
            
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompts = self._load_prompts()
        self.default_timeout = 60 # ì´ˆ ë‹¨ìœ„ íƒ€ì„ì•„ì›ƒ ê¸°ë³¸ê°’

    def is_api_key_configured(self, model_type: Optional[str] = None) -> bool:
        """ì§€ì •ëœ ëª¨ë¸ ë˜ëŠ” ì „ì²´ AI ì„œë¹„ìŠ¤ API í‚¤ ì„¤ì • ì—¬ë¶€ í™•ì¸"""
        if model_type == 'openai':
            return bool(self.openai_api_key)
        if model_type == 'claude':
            return bool(self.claude_api_key and CLAUDE_AVAILABLE and self.claude_client)
        if model_type == 'gemini':
            return bool(self.gemini_api_key and GEMINI_AVAILABLE and self.gemini_model)

        # íŠ¹ì • ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ í•˜ë‚˜ë¼ë„ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        return bool(self.openai_api_key or \
                    (self.claude_api_key and CLAUDE_AVAILABLE and self.claude_client) or \
                    (self.gemini_api_key and GEMINI_AVAILABLE and self.gemini_model))

    def _load_prompts(self) -> Dict[str, str]:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ"""
        return {
            'analysis': """ë‹¹ì‹ ì€ ì „ë¬¸ CAD ë„ë©´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ DXF ë„ë©´ ë¶„ì„ ë°ì´í„°ë¥¼ ê²€í† í•˜ê³  ì „ë¬¸ê°€ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”:

{analysis_data}

ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ë„ë©´ í’ˆì§ˆ í‰ê°€
2. ì ì¬ì  ë¬¸ì œì 
3. ê°œì„  ì œì•ˆì‚¬í•­
4. ì—…ê³„ í‘œì¤€ ì¤€ìˆ˜ ì—¬ë¶€
5. ì„¤ê³„ íš¨ìœ¨ì„±

ì „ë¬¸ê°€ ë‹µë³€:""",

            'comparison': """ë‘ DXF ë„ë©´ì˜ ë¹„êµ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

{comparison_data}

ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì£¼ìš” ë³€ê²½ì‚¬í•­ì˜ ì˜ë¯¸
2. ì„¤ê³„ ê°œì„  ì—¬ë¶€
3. ì ì¬ì  ë¬¸ì œë‚˜ ìœ„í—˜
4. ë²„ì „ ê´€ë¦¬ ì œì•ˆ

ì „ë¬¸ê°€ ì˜ê²¬:""",

            'autofix': """ë‹¤ìŒ DXF ë„ë©´ì˜ ë¬¸ì œì ë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:

{issues_data}

ê° ë¬¸ì œì— ëŒ€í•´:
1. ë¬¸ì œì˜ ì‹¬ê°ë„ í‰ê°€
2. ê¶Œì¥ ìˆ˜ì • ë°©ë²•
3. ìˆ˜ì • ìš°ì„ ìˆœìœ„
4. ì˜ˆë°© ë°©ë²•

ì „ë¬¸ê°€ ì¡°ì–¸:""",

            'design_review': """DXF ë„ë©´ì˜ ì„¤ê³„ ê²€í† ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤:

{design_data}

ê²€í†  í•­ëª©:
1. ì„¤ê³„ ë¬´ê²°ì„±
2. ì œì¡° ê°€ëŠ¥ì„±
3. ë¹„ìš© íš¨ìœ¨ì„±
4. ì•ˆì „ì„± ê³ ë ¤ì‚¬í•­
5. ìµœì í™” ê°€ëŠ¥ì„±

ì„¤ê³„ ê²€í†  ì˜ê²¬:""",

            'cnc_analysis': """ë‹¹ì‹ ì€ CNC ê°€ê³µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ DXF ë„ë©´ì„ CNC ê°€ê³µ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:

{analysis_data}

ë‹¤ìŒì„ í‰ê°€í•´ì£¼ì„¸ìš”:
1. ê°€ê³µì„± (Machinability) í‰ê°€
2. ê¶Œì¥ ê³µêµ¬ ë° ê°€ê³µ ì¡°ê±´
3. ì˜ˆìƒ ê°€ê³µ ì‹œê°„
4. ì ì¬ì  ê°€ê³µ ë¬¸ì œì 
5. ê³µêµ¬ ê²½ë¡œ ìµœì í™” ì œì•ˆ

CNC ì „ë¬¸ê°€ ë¶„ì„:""",

            'cost_estimation': """ë‹¹ì‹ ì€ ì œì¡° ë¹„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ DXF ë„ë©´ì˜ ì œì¡° ë¹„ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

{analysis_data}

ë‹¤ìŒì„ ì¶”ì •í•´ì£¼ì„¸ìš”:
1. ì¬ë£Œ ë¹„ìš©
2. ê°€ê³µ ì‹œê°„ ë° ì¸ê±´ë¹„
3. ê³µêµ¬ ë° ì†Œëª¨í’ˆ ë¹„ìš©
4. ì„¤ì • ì‹œê°„ (Setup time)
5. ì´ ì œì¡° ë¹„ìš© ë²”ìœ„

ë¹„ìš© ë¶„ì„:"""
        }
    
    async def analyze_with_openai(self, analysis_data: Dict, prompt_type: str = 'analysis') -> Dict:
        """OpenAIë¥¼ ì‚¬ìš©í•œ ë¶„ì„"""
        if not self.openai_api_key:
            return {'error': 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}
        
        try:
            # ë¶„ì„ ë°ì´í„°ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½
            summary = self._prepare_data_for_ai(analysis_data)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompts[prompt_type].format(
                analysis_data=json.dumps(summary, ensure_ascii=False, indent=2)
            )
            
            # OpenAI API í˜¸ì¶œ
            response = await self._call_openai_async(prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            ai_analysis = {
                'model': 'gpt-4',
                'timestamp': datetime.now().isoformat(),
                'prompt_type': prompt_type,
                'analysis': response,
                'recommendations': self._extract_recommendations(response),
                'issues': self._extract_issues(response)
            }
            
            return ai_analysis
            
        except Exception as e:
            logger.error(f"OpenAI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    async def analyze_with_claude(self, analysis_data: Dict, prompt_type: str = 'analysis') -> Dict:
        """Claudeë¥¼ ì‚¬ìš©í•œ ë¶„ì„"""
        if not self.claude_client:
            return {'error': 'Claude APIê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.'}
        
        try:
            # ë¶„ì„ ë°ì´í„° ì¤€ë¹„
            summary = self._prepare_data_for_ai(analysis_data)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompts[prompt_type].format(
                analysis_data=json.dumps(summary, ensure_ascii=False, indent=2)
            )
            
            # Claude API í˜¸ì¶œ
            response = await self._call_claude_async(prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            ai_analysis = {
                'model': 'claude-3',
                'timestamp': datetime.now().isoformat(),
                'prompt_type': prompt_type,
                'analysis': response,
                'recommendations': self._extract_recommendations(response),
                'issues': self._extract_issues(response)
            }
            
            return ai_analysis
            
        except Exception as e:
            logger.error(f"Claude ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    async def analyze_with_gemini(self, analysis_data: Dict, prompt_type: str = 'analysis') -> Dict:
        """Geminië¥¼ ì‚¬ìš©í•œ ë¶„ì„"""
        if not self.gemini_model:
            return {'error': 'Gemini APIê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.'}
        
        try:
            # ë¶„ì„ ë°ì´í„° ì¤€ë¹„
            summary = self._prepare_data_for_ai(analysis_data)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompts[prompt_type].format(
                analysis_data=json.dumps(summary, ensure_ascii=False, indent=2)
            )
            
            # Gemini API í˜¸ì¶œ
            response = await self._call_gemini_async(prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            ai_analysis = {
                'model': 'gemini-pro',
                'timestamp': datetime.now().isoformat(),
                'prompt_type': prompt_type,
                'analysis': response,
                'recommendations': self._extract_recommendations(response),
                'issues': self._extract_issues(response)
            }
            
            return ai_analysis
            
        except Exception as e:
            logger.error(f"Gemini ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    async def analyze_with_all(self, analysis_data: Dict, prompt_type: str = 'analysis') -> Dict:
        """ëª¨ë“  AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ë¶„ì„"""
        tasks = []
        
        if self.openai_api_key:
            tasks.append(self.analyze_with_openai(analysis_data, prompt_type))
            
        if self.claude_client:
            tasks.append(self.analyze_with_claude(analysis_data, prompt_type))
            
        if self.gemini_model:
            tasks.append(self.analyze_with_gemini(analysis_data, prompt_type))
        
        if not tasks:
            return {'error': 'AI APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}
        
        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        combined_analysis = {
            'timestamp': datetime.now().isoformat(),
            'models_used': [],
            'analyses': []
        }
        
        for result in results:
            if isinstance(result, dict) and 'error' not in result:
                combined_analysis['models_used'].append(result.get('model'))
                combined_analysis['analyses'].append(result)
        
        # í†µí•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        if len(combined_analysis['analyses']) > 1:
            combined_analysis['combined_insights'] = self._combine_insights(
                combined_analysis['analyses']
            )
        
        return combined_analysis
    
    def _prepare_data_for_ai(self, analysis_data: Dict) -> Dict:
        """AI ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ (í† í° ì œí•œ ê³ ë ¤)"""
        summary = {
            'file_info': analysis_data.get('file_info', {}),
            'summary': analysis_data.get('summary', {}),
            'entity_breakdown': analysis_data.get('entity_breakdown', {}),
            'layer_count': len(analysis_data.get('layers', [])),
            'total_entities': analysis_data.get('total_entities', 0)
        }
        
        # ê³ ê¸‰ ë¶„ì„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if 'quality_score' in analysis_data:
            summary['quality'] = {
                'score': analysis_data.get('quality_score'),
                'grade': analysis_data.get('quality_grade')
            }
            
        if 'anomalies' in analysis_data:
            summary['anomaly_count'] = len(analysis_data.get('anomalies', []))
            summary['anomaly_types'] = list(set(
                a.get('type') for a in analysis_data.get('anomalies', [])
            ))
        
        return summary
    
    async def _call_openai_async(self, prompt: str) -> str:
        """ë¹„ë™ê¸° OpenAI API í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ ë° ìƒì„¸ ì˜¤ë¥˜ ì²˜ë¦¬ ì¶”ê°€)"""
        if not self.is_api_key_configured('openai'):
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        try:
            api_call_task = asyncio.to_thread(
                openai.ChatCompletion.create, # openai ë¼ì´ë¸ŒëŸ¬ë¦¬ v0.x.x ê¸°ì¤€
                                             # openai v1.x.x ì´ìƒì€ openai.chat.completions.create ì‚¬ìš©
                model="gpt-4", # ë˜ëŠ” "gpt-3.5-turbo" ë“± ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ CAD ë„ë©´ ë¶„ì„ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            response = await asyncio.wait_for(api_call_task, timeout=self.default_timeout)
            return response.choices[0].message.content

        except openai.error.AuthenticationError as e:
            logger.error(f"OpenAI API ì¸ì¦ ì˜¤ë¥˜: {e}")
            raise ConnectionRefusedError(f"OpenAI API ì¸ì¦ ì‹¤íŒ¨: {e}") # ì¢€ ë” ì¼ë°˜ì ì¸ ì˜ˆì™¸ë¡œ ë³€í™˜ ê°€ëŠ¥
        except openai.error.RateLimitError as e:
            logger.error(f"OpenAI API ì†ë„ ì œí•œ ì˜¤ë¥˜: {e}")
            raise ConnectionAbortedError(f"OpenAI API ì†ë„ ì œí•œ ì´ˆê³¼: {e}") # ì¢€ ë” ì¼ë°˜ì ì¸ ì˜ˆì™¸ë¡œ ë³€í™˜ ê°€ëŠ¥
        except openai.error.APIError as e:
            logger.error(f"OpenAI API ì¼ë°˜ ì˜¤ë¥˜: {e}")
            raise RuntimeError(f"OpenAI API ì˜¤ë¥˜: {e}")
        except asyncio.TimeoutError:
            logger.error("OpenAI API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼")
            raise TimeoutError("OpenAI API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼")
        except Exception as e: # ê·¸ ì™¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜
            logger.error(f"OpenAI API í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            raise RuntimeError(f"OpenAI API í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
    
    async def _call_claude_async(self, prompt: str) -> str:
        """ë¹„ë™ê¸° Claude API í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ ë° ìƒì„¸ ì˜¤ë¥˜ ì²˜ë¦¬ ì¶”ê°€)"""
        if not self.is_api_key_configured('claude'):
            raise ValueError("Claude API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        try:
            api_call_task = asyncio.to_thread(
                self.claude_client.messages.create,
                model="claude-3-sonnet-20240229", # ë˜ëŠ” ë‹¤ë¥¸ claude ëª¨ë¸
                max_tokens=1500,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            response = await asyncio.wait_for(api_call_task, timeout=self.default_timeout)
            return response.content[0].text
            
        except anthropic.APIConnectionError as e:
            logger.error(f"Claude API ì—°ê²° ì˜¤ë¥˜: {e}")
            raise ConnectionError(f"Claude API ì—°ê²° ì‹¤íŒ¨: {e}")
        except anthropic.RateLimitError as e:
            logger.error(f"Claude API ì†ë„ ì œí•œ ì˜¤ë¥˜: {e}")
            raise ConnectionAbortedError(f"Claude API ì†ë„ ì œí•œ ì´ˆê³¼: {e}")
        except anthropic.APIStatusError as e: # ì¼ë°˜ì ì¸ API ì˜¤ë¥˜ (HTTP ìƒíƒœ ì½”ë“œ 4xx, 5xx ë“±)
            logger.error(f"Claude API ìƒíƒœ ì˜¤ë¥˜ (status_code={e.status_code}): {e.message}")
            raise RuntimeError(f"Claude API ì˜¤ë¥˜ (status={e.status_code}): {e.message}")
        except asyncio.TimeoutError:
            logger.error("Claude API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼")
            raise TimeoutError("Claude API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            logger.error(f"Claude API í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            raise RuntimeError(f"Claude API í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
    
    async def _call_gemini_async(self, prompt: str) -> str:
        """ë¹„ë™ê¸° Gemini API í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ ë° ìƒì„¸ ì˜¤ë¥˜ ì²˜ë¦¬ ì¶”ê°€)"""
        if not self.is_api_key_configured('gemini'):
            raise ValueError("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ëª¨ë¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        try:
            api_call_task = asyncio.to_thread(
                self.gemini_model.generate_content,
                prompt,
                generation_config=genai.types.GenerationConfig(
                    candidate_count=1,
                    max_output_tokens=1500,
                    temperature=0.7
                )
            )
            response = await asyncio.wait_for(api_call_task, timeout=self.default_timeout)
            return response.text
            
        except google.api_core.exceptions.GoogleAPIError as e: # Gemini APIì˜ ì¼ë°˜ì ì¸ ì˜ˆì™¸
            logger.error(f"Gemini API ì˜¤ë¥˜: {e}")
            if isinstance(e, google.api_core.exceptions.PermissionDenied):
                 raise ConnectionRefusedError(f"Gemini API ì ‘ê·¼ ê±°ë¶€: {e}")
            elif isinstance(e, google.api_core.exceptions.ResourceExhausted):
                 raise ConnectionAbortedError(f"Gemini API ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ (ì†ë„ ì œí•œ ë“±): {e}")
            raise RuntimeError(f"Gemini API ì˜¤ë¥˜: {e}")
        except asyncio.TimeoutError:
            logger.error("Gemini API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼")
            raise TimeoutError("Gemini API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            logger.error(f"Gemini API í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            raise RuntimeError(f"Gemini API í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
    
    def _extract_recommendations(self, ai_response: str) -> List[str]:
        """AI ì‘ë‹µì—ì„œ ê¶Œì¥ì‚¬í•­ ì¶”ì¶œ"""
        recommendations = []
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ê¶Œì¥ì‚¬í•­ ì¶”ì¶œ
        lines = ai_response.split('\n')
        in_recommendations = False
        
        for line in lines:
            if any(keyword in line.lower() for keyword in ['ê¶Œì¥', 'ì œì•ˆ', 'ì¶”ì²œ', 'recommend']):
                in_recommendations = True
                continue
                
            if in_recommendations and line.strip():
                if line.strip().startswith(('-', '*', 'â€¢', '1', '2', '3')):
                    recommendations.append(line.strip().lstrip('-*â€¢0123456789. '))
                elif len(recommendations) > 0:
                    break
        
        return recommendations[:5]  # ìƒìœ„ 5ê°œë§Œ
    
    def _extract_issues(self, ai_response: str) -> List[str]:
        """AI ì‘ë‹µì—ì„œ ë¬¸ì œì  ì¶”ì¶œ"""
        issues = []
        
        lines = ai_response.split('\n')
        in_issues = False
        
        for line in lines:
            if any(keyword in line.lower() for keyword in ['ë¬¸ì œ', 'ì´ìŠˆ', 'issue', 'problem']):
                in_issues = True
                continue
                
            if in_issues and line.strip():
                if line.strip().startswith(('-', '*', 'â€¢', '1', '2', '3')):
                    issues.append(line.strip().lstrip('-*â€¢0123456789. '))
                elif len(issues) > 0:
                    break
        
        return issues[:5]
    
    def _combine_insights(self, analyses: List[Dict]) -> Dict:
        """ì—¬ëŸ¬ AI ë¶„ì„ ê²°ê³¼ í†µí•©"""
        combined = {
            'common_issues': [],
            'common_recommendations': [],
            'confidence_level': 'high' if len(analyses) > 1 else 'medium'
        }
        
        # ê³µí†µ ì´ìŠˆ ì°¾ê¸°
        all_issues = []
        for analysis in analyses:
            all_issues.extend(analysis.get('issues', []))
        
        # ì¤‘ë³µ ì œê±°í•˜ê³  ë¹ˆë„ìˆœ ì •ë ¬
        issue_counts = {}
        for issue in all_issues:
            # ìœ ì‚¬í•œ ì´ìŠˆ ê·¸ë£¹í™” (ê°„ë‹¨í•œ êµ¬í˜„)
            key = issue.lower()[:30]
            issue_counts[key] = issue_counts.get(key, 0) + 1
        
        combined['common_issues'] = [
            issue for issue, count in sorted(
                issue_counts.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:3]
        ]
        
        # ê³µí†µ ê¶Œì¥ì‚¬í•­ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        all_recommendations = []
        for analysis in analyses:
            all_recommendations.extend(analysis.get('recommendations', []))
        
        rec_counts = {}
        for rec in all_recommendations:
            key = rec.lower()[:30]
            rec_counts[key] = rec_counts.get(key, 0) + 1
        
        combined['common_recommendations'] = [
            rec for rec, count in sorted(
                rec_counts.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:3]
        ]
        
        return combined
    
    def generate_ai_report(self, ai_analysis: Dict) -> str:
        """AI ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = "# ğŸ¤– AI ë„ë©´ ë¶„ì„ ë¦¬í¬íŠ¸\n\n"
        
        if 'error' in ai_analysis:
            report += f"âš ï¸ ì˜¤ë¥˜: {ai_analysis['error']}\n"
            return report
        
        # ë‹¨ì¼ ëª¨ë¸ ë¶„ì„
        if 'model' in ai_analysis:
            report += f"**ë¶„ì„ ëª¨ë¸**: {ai_analysis['model']}\n"
            report += f"**ë¶„ì„ ì‹œê°„**: {ai_analysis['timestamp']}\n\n"
            
            report += "## ğŸ“Š ë¶„ì„ ê²°ê³¼\n"
            report += ai_analysis.get('analysis', 'ë¶„ì„ ê²°ê³¼ ì—†ìŒ')
            report += "\n\n"
            
            if ai_analysis.get('issues'):
                report += "## ğŸš¨ ë°œê²¬ëœ ë¬¸ì œì \n"
                for issue in ai_analysis['issues']:
                    report += f"- {issue}\n"
                report += "\n"
            
            if ai_analysis.get('recommendations'):
                report += "## ğŸ’¡ ê¶Œì¥ì‚¬í•­\n"
                for rec in ai_analysis['recommendations']:
                    report += f"- {rec}\n"
                report += "\n"
        
        # ë³µìˆ˜ ëª¨ë¸ ë¶„ì„
        elif 'analyses' in ai_analysis:
            report += f"**ì‚¬ìš© ëª¨ë¸**: {', '.join(ai_analysis.get('models_used', []))}\n"
            report += f"**ë¶„ì„ ì‹œê°„**: {ai_analysis['timestamp']}\n\n"
            
            # ê° ëª¨ë¸ë³„ ë¶„ì„
            for i, analysis in enumerate(ai_analysis['analyses'], 1):
                report += f"### {analysis['model']} ë¶„ì„\n"
                report += analysis.get('analysis', '')[:500] + "...\n\n"
            
            # í†µí•© ì¸ì‚¬ì´íŠ¸
            if 'combined_insights' in ai_analysis:
                insights = ai_analysis['combined_insights']
                
                report += "## ğŸ” í†µí•© ì¸ì‚¬ì´íŠ¸\n"
                
                if insights.get('common_issues'):
                    report += "\n### ê³µí†µ ë¬¸ì œì \n"
                    for issue in insights['common_issues']:
                        report += f"- {issue}\n"
                
                if insights.get('common_recommendations'):
                    report += "\n### ê³µí†µ ê¶Œì¥ì‚¬í•­\n"
                    for rec in insights['common_recommendations']:
                        report += f"- {rec}\n"
                
                report += f"\n**ì‹ ë¢°ë„**: {insights.get('confidence_level', 'medium').upper()}\n"
        
        return report
    
    async def interactive_chat(self, question: str, context: Dict) -> str:
        """ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ"""
        prompt = f"""DXF ë„ë©´ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸:
{json.dumps(self._prepare_data_for_ai(context), ensure_ascii=False, indent=2)}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ì „ë¬¸ê°€ ë‹µë³€:"""
        
        if self.openai_api_key:
            try:
                response = await self._call_openai_async(prompt)
                return response
            except:
                pass
        
        if self.claude_client:
            try:
                response = await self._call_claude_async(prompt)
                return response
            except:
                pass
        
        return "AI APIê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤." 